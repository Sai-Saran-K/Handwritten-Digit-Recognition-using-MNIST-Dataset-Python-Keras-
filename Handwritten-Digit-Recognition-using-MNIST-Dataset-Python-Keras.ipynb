{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwZCVyfbptbv"
      },
      "source": [
        "## Handwritten Digit Recognition - MNIST Dataset - CNN - Python/Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM-OLuTRptb1"
      },
      "source": [
        "This is a Multi-Class Classification problem (10 classes)\n",
        "- Language: Python\n",
        "- Deep Learning Package: Keras\n",
        "- Dataset: MNIST dataset available with Keras\n",
        "- Model: MLP, CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iyb1PQIaptb2"
      },
      "source": [
        "#### Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPnT_RX5ptb3"
      },
      "source": [
        "Load train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXuyQAyyptb3",
        "outputId": "83b878a7-6b09-43da-9c26-c8987df4266d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StqDDcE1ptb5"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUljWENuptb5",
        "outputId": "c328c1ed-dcc0-4fb2-b770-900c1571cb80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1eRYV0Mptb5",
        "outputId": "d5336986-824d-4a33-e88c-e555e0cf51ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7eHR2oWptb6",
        "outputId": "3f7f29c5-070e-4aec-cbe9-b0866f6a02c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xLDjXDyptb6",
        "outputId": "149f762c-aaf6-40c7-e183-5964944a94f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paZW22__ptb6"
      },
      "source": [
        "We have following with us:\n",
        "\n",
        "<br>**Training Set**\n",
        "- 60000 images\n",
        "- Each image is of the shape 28 x 28 (rows x columns)\n",
        "- 60000 labels defining the digit that corresponds to the respective image\n",
        "\n",
        "<br>**Test Set**\n",
        "- 10000 images\n",
        "- Each image is of the shape 28 x 28 (rows x columns)\n",
        "- 10000 labels defining the digit that corresponds to the respective image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c7b3-2Bptb6"
      },
      "source": [
        "#### Dataset Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMn4f8_nptb7"
      },
      "source": [
        "Let us visualize the handwritten digit images and labels for 0th training sample. We can see that the 0th image shows a handwritten 5 and the 0th label has the value 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwmOxni5ptb7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LvopPvEptb7",
        "outputId": "4640d1c1-16a8-4fd8-f536-08d1caa83e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.gray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXLfIsmptb7",
        "outputId": "0807841c-0962-42d6-f2fd-2b432f55d23c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x13528bbde10>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(train_images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDOviDTXptb8",
        "outputId": "5eb3d972-d9d6-4743-d47b-7eec99de9097"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFwCkk25ptb8"
      },
      "source": [
        "### MLP based solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZx0PHxgptb8"
      },
      "source": [
        "#### Define the model\n",
        "- Input Shape: Column Vector corresponding to 28x28 image matrix for the digit = 784 rows,1 column = 784,1\n",
        "- 1st hidden layer: Number of neurons = 512\n",
        "- 1st hidden layer: Activation Function = relu (for non-linearity detection)\n",
        "- Output layer: Number of neurons = 10 (corresponding to 0 to 9 digits)\n",
        "- Output layer: Activation Function = softmax (to get probabilities for the repective 10 classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSV8C5nqptb8"
      },
      "outputs": [],
      "source": [
        "from keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDpOJrS2ptb9"
      },
      "outputs": [],
      "source": [
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGWPr2Ptptb9"
      },
      "outputs": [],
      "source": [
        "model_mlp = models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FdMvowLptb9"
      },
      "outputs": [],
      "source": [
        "model_mlp.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8d8qpeptb9"
      },
      "outputs": [],
      "source": [
        "model_mlp.add(layers.Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQ6RTjEWptb9",
        "outputId": "104c0228-9e92-4560-af51-32ea9394ada5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_mlp.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po7TYoPqptb-"
      },
      "source": [
        "#### Define the optimizer function, loss function and metrics to be used for the model.\n",
        "- Going ahead with the well known functions at this point in time\n",
        "- Selected accuracy as the metrics to understand validation / test accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPwlDMFyptb-"
      },
      "outputs": [],
      "source": [
        "model_mlp.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVgEpZrjptb_"
      },
      "source": [
        "#### We will preprocess the data before feeding it to the model:\n",
        "- Change the images to column vector form: 28x28 -> 784x1 to match to model's input expectations\n",
        "- Change the vector values from int to float: to get continuos values as we move thro's model's layers\n",
        "- Scale the vector values to be in the [0,1] interval: model will see all samples with equal weightage as the range of values for all samples are same.\n",
        "\n",
        "Note: gray scale values will be from 0 to 255... hence dividing the float by 255 will give us the values in [0,1] interval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3lYq-jjptb_"
      },
      "source": [
        "#### Data Preprocessing - Train Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z628GSdptb_",
        "outputId": "b51d3458-a138-46d6-c028-8fe66a469b1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nx-dudfgptb_"
      },
      "outputs": [],
      "source": [
        "train_images_mlp = train_images.reshape(60000, 28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPpgENavptb_",
        "outputId": "b08623e3-5900-4a17-ef21-779001b66433"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images_mlp.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSUo2eCfptcA",
        "outputId": "971e07e9-1c6e-464b-ae27-1672e7715d4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.uint8"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# type is an int\n",
        "# chose a non-zero value element ... somewhere in the middle of the column vector\n",
        "type(train_images_mlp[0][350])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htJIEu5dptcA",
        "outputId": "011f577c-ce15-4d23-ec5a-d820bc0a43ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images_mlp[0][350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvhZ2zTuptcA"
      },
      "outputs": [],
      "source": [
        "train_images_mlp = train_images_mlp.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjf8QA6gptcA",
        "outputId": "70d7272c-7e2b-47c1-d3d5-fab84e1f0624"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.float32"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# type has changed to float\n",
        "type(train_images_mlp[0][350])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGGMt9rHptcB",
        "outputId": "6fb0479a-b678-4694-a790-8d4945c304cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.27450982"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images_mlp[0][350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22aFLeVoptcB",
        "outputId": "c9f0b45c-20ee-47de-f21b-8bd76ed321d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.27450980392156865"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "70/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Utv-a4OrptcB"
      },
      "source": [
        "#### Data Preprocessing - Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFbunQ1hptcB"
      },
      "outputs": [],
      "source": [
        "test_images_mlp = test_images.reshape(10000,28*28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsKjvBgvptcC"
      },
      "outputs": [],
      "source": [
        "test_images_mlp = test_images_mlp.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQXwysWCptcC"
      },
      "source": [
        "#### We will process the data labels:\n",
        "- We have ten output classes ... consider it as a 10bit output\n",
        "- We need to represent the label as a 10bit value where bit corresponding to the digit value will be 1 and rest all 9 bits will be 0\n",
        "- Say label = 5. We will change it to a 10bit value as 0000010000 (note as the index starts at 0, 6th element corresponds to digit 5 and hence is 1 and rest are zero)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzuhcIygptcC"
      },
      "source": [
        "#### Data Preprocessing - Train Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zO9Dhe5NptcC",
        "outputId": "3692b129-cc83-4eb2-b95f-f0a8c9ff2b7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIYJ3ihJptcC"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywQ1-X57ptcC"
      },
      "outputs": [],
      "source": [
        "train_labels_mlp = to_categorical(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2PpMRxhptcD",
        "outputId": "701bd1da-9e10-4924-e437-c5c1318728ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels_mlp[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGVyDUaYptcD"
      },
      "source": [
        "#### Data Preprocessing - Train Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2hpK2cKptcD"
      },
      "outputs": [],
      "source": [
        "test_labels_mlp = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUaXivRmptcD"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmDto3bVptcE"
      },
      "source": [
        "We will now train the model using train images and train labels.\n",
        "- We will use a batch size = 120.\n",
        "- 1 epoch = 60000 / 120 = 500 batches\n",
        "- 1 epoch = 1 complete run of all train samples for training the model\n",
        "- We will go for a total of 5 epochs = 5 complete run of the all train samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YLd8hC1ptcE",
        "outputId": "1accd4cd-14a6-45fe-9382-d98b7302d25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.2532 - accuracy: 0.9264\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.1022 - accuracy: 0.9694\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0677 - accuracy: 0.9795\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0495 - accuracy: 0.9852\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0371 - accuracy: 0.9890\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x13528c2c198>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_mlp.fit(train_images_mlp, train_labels_mlp, epochs = 5, batch_size = 120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vIkVgR2ptcF"
      },
      "source": [
        "At this run, we got a training accuracy of **~98.9%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok2F1FkmptcF"
      },
      "source": [
        "#### Test the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcZHQWCLptcF"
      },
      "source": [
        "We will now test model's performance with the test data.\n",
        "- We predict the class for each of the 10000 test using the model.\n",
        "- We will check the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtphJmVmptcG",
        "outputId": "ff267ba2-9bbd-4560-d2ff-5848792fa850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 56us/step\n"
          ]
        }
      ],
      "source": [
        "test_loss_mlp, test_acc_mlp = model_mlp.evaluate(test_images_mlp, test_labels_mlp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe3qIXPKptcG",
        "outputId": "ce149efb-d384-47e0-94db-aad11e939da0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy: 98.01999926567078\n"
          ]
        }
      ],
      "source": [
        "print('test accuracy:', (test_acc_mlp*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBZiZCrcptcH"
      },
      "source": [
        "At this run, we got a training accuracy of **~98.0%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzEXQuRcptcH"
      },
      "source": [
        "### CNN based solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqnvuQVtptcH"
      },
      "source": [
        "#### Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jSMw6TzptcH"
      },
      "outputs": [],
      "source": [
        "from keras import models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frFQf1eVptcH"
      },
      "outputs": [],
      "source": [
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXRsaAhvptcH"
      },
      "outputs": [],
      "source": [
        "model_cnn = models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZPG4zo-ptcH"
      },
      "source": [
        "Layer Details:\n",
        "- 2 dimensional Convolution Layer\n",
        "- Number of filters/kernels = 32\n",
        "- Filter/Kernel Size = 3x3\n",
        "- Activation Function = relu (for non-linearity detection)\n",
        "- Input Shape = 28x28 matrix with 1 channel (as image is gray scale, we have only 1 channel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7adwB9iIptcI"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i4hZyP5ptcI"
      },
      "source": [
        "Layer Details:\n",
        "- Downsample the output from previous layer\n",
        "- We will take the max value for a every 2x2 window ... moved over the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csnTn_K8ptcI"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.MaxPooling2D(2,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF01LM-tptcI"
      },
      "source": [
        "Layer Details:\n",
        "- 2 dimensional Convolution Layer\n",
        "- Number of filters/kernels = 64\n",
        "- Filter/Kernel Size = 3x3\n",
        "- Activation Function = relu (for non-linearity detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX07w0IlptcI"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Conv2D(64, (3,3), activation = 'relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNB8dv4cptcI"
      },
      "source": [
        "Layer Details:\n",
        "- Downsample the output from previous layer\n",
        "- We will take the max value for a every 2x2 window ... moved over the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c1ixhKtptcJ"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.MaxPooling2D(2,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV6rItzDptcJ"
      },
      "source": [
        "Layer Details:\n",
        "- 2 dimensional Convolution Layer\n",
        "- Number of filters/kernels = 64\n",
        "- Filter/Kernel Size = 3x3\n",
        "- Activation Function = relu (for non-linearity detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1rcG_7JptcJ"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Conv2D(64, (3,3), activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj7InVBptcJ"
      },
      "source": [
        "Data at this stage is in matrix form. We will convert it to vector form to feed to a fully connected network (FCN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-L6nqIoptcJ"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Flatten())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea9g8OslptcJ"
      },
      "source": [
        "We will design for 64 outputs with activation function as relu (to learn non-linearity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtCFUnE9ptcK"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Dense(64, activation = 'relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIeJyRiCptcK"
      },
      "source": [
        "This is the final layer. Hence, the outputs will be 10 corresponding to the 10 digits (0 to 9).\n",
        "Activation Function chosen here is softmax to have a probabilistic output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqsHFKLEptcK"
      },
      "outputs": [],
      "source": [
        "model_cnn.add(layers.Dense(10, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPK1vcJWptcK",
        "outputId": "aa6d5870-773e-47dc-9ae2-6091ee79b594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If9i2P5-ptcK"
      },
      "source": [
        "#### Data Preprocessing - Train and Test Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADYOBzEcptcK",
        "outputId": "c42bf687-4145-406c-bbcf-bd92f977e567"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IQ4tuVDptcK"
      },
      "source": [
        "CNN needs another dimension for the channel. Here as the image is gray scale it will be 1 channel. If we had color images, the channel value would have been 3 for the three channels - Red, Green and Blue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh--vxcMptcL"
      },
      "outputs": [],
      "source": [
        "train_images_cnn = train_images.reshape(60000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJDEOtI3ptcL",
        "outputId": "62efb161-3349-4567-dbe3-308700d024b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images_cnn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXihAyvYptcL"
      },
      "source": [
        "We need to change the element values from integer to decimal to have continuos values during prediction through the various layers. We will limit the values to the interval [0,1] so that the model treats each sample with equal weightage as the range of values for all samples will be fixed. We will do this by dividing the decimal values by 255 (gray scale values are from 0 to 255 ... 0 representing black to white)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYJTA8lcptcL"
      },
      "outputs": [],
      "source": [
        "train_images_cnn = train_images_cnn.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sa4d7EUfptcL"
      },
      "outputs": [],
      "source": [
        "test_images_cnn = test_images.reshape(10000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ida9crA4ptcL"
      },
      "outputs": [],
      "source": [
        "test_images_cnn = test_images_cnn.astype('float32') / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rr2V5rzDptcM"
      },
      "source": [
        "#### Data Preprocessing - Train and Test Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JpxSllYptcM"
      },
      "source": [
        "We will convert the labels to 10bit values. Only 1 of the bits of the 10bit value will be 1 corresponding to the location for the respective digit and rest all bits will be 0. This is required to match to the model's output layer expectation so that we can effectively train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVz8JlR5ptcM"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OTbLmpLptcM"
      },
      "outputs": [],
      "source": [
        "train_labels_cnn = to_categorical(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vLcPYHIptcM"
      },
      "outputs": [],
      "source": [
        "test_labels_cnn = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPFyXYLuptcM"
      },
      "source": [
        "#### Define the optimizer function, loss function and metrics to be used for the model.\n",
        "- Going ahead with the well known functions at this point in time\n",
        "- Selected accuracy as the metrics to understand validation / test accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BARIjsGIptcN"
      },
      "outputs": [],
      "source": [
        "model_cnn.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RVH1_sxptcN"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rwCqPERptcN"
      },
      "source": [
        "#### We will now train the model using train images and train labels.\n",
        "- We will use a batch size = 60.\n",
        "- 1 epoch = 60000 / 60 = 1000 batches\n",
        "- 1 epoch = 1 complete run of all train samples for training the model\n",
        "- We will go for a total of 5 epochs = 5 complete run of the all train samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PG_8xCgptcN",
        "outputId": "ad7e7113-68e4-4765-e764-24a8de03df31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 38s 636us/step - loss: 0.1731 - accuracy: 0.9459\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 38s 639us/step - loss: 0.0477 - accuracy: 0.9853\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 39s 647us/step - loss: 0.0327 - accuracy: 0.9898\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 40s 665us/step - loss: 0.0243 - accuracy: 0.9925\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 39s 645us/step - loss: 0.0198 - accuracy: 0.9941\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x1352a775c18>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_cnn.fit(train_images_cnn, train_labels_cnn, epochs = 5, batch_size = 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwT92fMyptcN"
      },
      "source": [
        "At this run, we got a training accuracy of **~99.4%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSG76OJPptcN"
      },
      "source": [
        "#### Test the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTs1IdCWptcO"
      },
      "source": [
        "We will now test model's performance with the test data.\n",
        "- We predict the class for each of the 10000 test using the model.\n",
        "- We will check the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYDEWkjqptcO",
        "outputId": "0b1e2674-646c-479f-cf2f-c2b5bef631f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 2s 181us/step\n"
          ]
        }
      ],
      "source": [
        "test_loss_cnn, test_acc_cnn = model_cnn.evaluate(test_images_cnn, test_labels_cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eCx8Ob8ptcO",
        "outputId": "3c6d0fdf-ca60-4ab5-fff0-a27da661a4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test accuracy: 99.26999807357788\n"
          ]
        }
      ],
      "source": [
        "print('test accuracy:', (test_acc_cnn*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2oV1_VkptcO"
      },
      "source": [
        "At this run, we got a training accuracy of **~99.3%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbttUZ_aptb1"
      },
      "source": [
        "#### Key Results\n",
        "**MLP based solution**\n",
        "- Input (784x1 column vector) -> 1 hidden layer (512 neurons, relu) -> Output (10 classes, softmax)\n",
        "- Number of parameters = **407050**\n",
        "- Training Accuracy = **~98.9%**\n",
        "- Test Accuracy = **~98.0%**\n",
        "\n",
        "\n",
        "**CNN based solution**\n",
        "- Input (28x28x1 matrix) -> Conv(32 3x3 filters, relu) -> Maxpool(2x2) -> Conv(64 3x3 filters, relu) -> Maxpool(2x2) -> Conv(64 3x3 filters, relu) -> Flatten to a column vector for FCN -> FCN (64 outputs, relu) -> FCN (10 outputs, softmax)\n",
        "- Number of parameters = **93322**\n",
        "- Training Accuracy = **~99.4%**\n",
        "- Test Accuracy = **~99.3%**\n",
        "\n",
        "#### Conclusion\n",
        "CNN has the advantage of using the spatial information in the image as compared to MLP, hence achieves better accuracy with reduced parameters."
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}